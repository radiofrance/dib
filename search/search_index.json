{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>dib is a tool designed to help build multiple Docker images defined within a directory, possibly having dependencies with one another, in a single command.</p> <p>Warning</p> <p>dib is still at an early stage, development is still ongoing and new minor releases may bring some breaking changes.  This may occur until we release the v1.</p>"},{"location":"#purpose","title":"Purpose","text":"<p>As containers have become the standard software packaging technology, we have to deal with an ever-increasing number of  image definitions. In DevOps teams especially, we need to manage dozens of Dockerfiles, and the monorepo is often the  solution of choice to store and version them.</p> <p>We use CI/CD pipelines to help by automatically building and pushing the images to a registry, but it's often  inefficient as all the images are rebuilt at every commit/pull request. There are possible solutions to optimize this, like changesets detection or build cache persistence to increase  efficiency, but it's not an easy task.</p> <p>Also, being able to test and validate the produced images was also something that we were looking forward to.</p> <p>dib was created to solve these issues, and manage a large number of images in the most efficient way as possible.</p>"},{"location":"#concepts","title":"Concepts","text":"<p>Before using dib, there are important basic concepts to know about, to understand how it works internally.</p>"},{"location":"#build-directory","title":"Build directory","text":"<p>dib needs a path to a root directory containing all the images it should manage. The structure of this directory is not  important, dib will discover all the Dockerfiles within it recursively.</p> <p>Example with a simple directory structure: <pre><code>images/\n\u251c\u2500\u2500 alpine\n|   \u2514\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 debian\n    \u251c\u2500\u2500 bookworm\n    |   \u2514\u2500\u2500 Dockerfile\n    \u2514\u2500\u2500 bullseye\n        \u2514\u2500\u2500 Dockerfile\n</code></pre></p> <p>Every Dockerfile must contain either a <code>name</code> or a <code>skipbuild</code> label: <pre><code>LABEL name=\"alpine\"\n</code></pre> OR <pre><code>LABEL skipbuild=\"true\"\n</code></pre></p> <p>If the <code>skipbuild</code> label is used, the image will be ignored and dib won't manage it. The <code>name</code> label value must be unique within the build directory.</p> <p>A <code>.dockerignore</code> file can be added to any directory and is used to exclude files from the build context of the same directory.</p> <p>Any other file in the build directory is considered as a build context for the image it belongs to.</p>"},{"location":"#dependency-graph","title":"Dependency Graph","text":"<p>Because some images may depend on other images (when a <code>FROM</code> statement references an image also defined within the  build directory), dib internally builds a graph of dependencies (DAG). During the build process, dib waits until all parent images finish to build before building the children.</p> <p>Example dependency graph: <pre><code>graph LR\n  A[alpine] --&gt; B[nodejs];\n  B --&gt; C[foo];\n  D[debian] --&gt; E[bar];\n  B --&gt; E;</code></pre></p> <p>In this example, dib will wait for the <code>alpine</code> image to be built before proceeding to <code>nodejs</code>, and then both <code>alpine</code> and <code>bullseye</code> can be built in parallel (see the <code>--rate-limit</code> build option).</p> <p>Once <code>debian</code> is completed, the build of <code>bar</code> begins, and as soon as <code>nodejs</code> is completed, <code>foo</code> follows.</p>"},{"location":"#image-version-tag","title":"Image Version Tag","text":"<p>dib only builds an image when something has changed in its build context since the last build. To track the changes, dib computes a checksum of all the files in the context, and generates a human-readable tag out of it. If any file changes in the build context (or in the build context of any parent image), the computed human-readable tag changes as  well.</p> <p>dib knows it needs to rebuild an image if the target tag is not present in the registry.</p>"},{"location":"#placeholder-tag","title":"Placeholder Tag","text":"<p>When updating images having children, dib needs to update the tags in <code>FROM</code> statements in all child images before running the build, to match the newly computed tag. </p> <p>Example:</p> <p>Given a parent image named \"parent\": <pre><code>LABEL name=\"parent\"\n</code></pre></p> <p>And a child image referencing the parent: <pre><code>FROM registry.example.com/parent:REPLACE_ME\nLABEL name=\"child\"\n</code></pre></p> <p>When we build using the same placeholder tag: <pre><code>dib build \\\n  --registry-url=registry.example.com \\\n  --placeholder-tag=REPLACE_ME\n</code></pre></p> <p>Then any change to the parent image will be inherited by the child. By default, the placeholder tag is <code>latest</code>.</p> <p>In some cases, we want to be able to freeze the version of the parent image to a specific tag. To do so, just change the tag in the <code>FROM</code> statement to be anything else than the placeholder tag: <pre><code>FROM registry.example.com/parent:some-specific-tag\nLABEL name=\"child\"\n</code></pre></p> <p>Then any change to the parent image will not be inherited by the child.</p>"},{"location":"#tag-promotion","title":"Tag promotion","text":"<p>dib always tries to build and push images when it detects some changes, by it doesn't move the reference tag  (<code>latest</code> by default) to the latest version. This allows dib to run on feature branches without interfering with one another. Once the changes are satisfying, just re-run dib with the <code>--release</code> flag to promote the current version with the reference tag.</p> <p>Example workflow</p> <p>Let's assume we have a simple GitFlow setup, with CI/CD pipelines running on each commit to build docker images with dib.</p> <p>When one creates a branch from the main branch, and commits some changes to an image. dib builds and pushes the <code>cat-south</code> tag, but <code>latest</code> still references the same tag (<code>beacon-two</code>):</p> <pre><code>gitGraph\n       commit id: \"autumn-golf\"\n       commit id: \"beacon-two\" tag: \"latest\"\n       branch feature\n       commit id: \"cat-south\"</code></pre> <p>Once the feature branch gets merged, the <code>cat-south</code> tag is promoted to <code>latest</code>: <pre><code>gitGraph\n       commit id: \"autumn-golf\"\n       commit id: \"beacon-two\"\n       branch feature\n       commit id: \"cat-south\"\n       checkout main\n       merge feature\n       commit id: \"cat-south \" tag: \"latest\"</code></pre></p>"},{"location":"#license","title":"License","text":"<p>dib is licensed under the CeCILL V2.1 License</p>"},{"location":"backends/","title":"Build Backends","text":"<p>The build backend is a software or service responsible for actually building the images. dib itself is not capable of building images, it delegates this part to the build backend.</p> <p>dib supports multiple build backends. Currently, available backends are <code>docker</code>, <code>kaniko</code>, and <code>buildkit</code>. You can select the  backend to use with the <code>--backend</code> option. Note that <code>docker</code> and <code>kaniko</code> backends are deprecated and will be removed in a future release. <code>buildkit</code> is now the recommended and default backend.</p> <p>Executor compatibility matrix</p> Backend Local Docker Kubernetes Docker \u2714 \u2717 \u2717 Kaniko \u2717 \u2714 \u2714 BuildKit \u2714 \u2717 \u2714"},{"location":"backends/#docker","title":"Docker","text":"<p>Deprecated: The Docker backend is deprecated and will be removed in a future release. Please use the BuildKit backend instead.</p> <p>The <code>docker</code> backend uses Docker behind the scenes, and runs <code>docker build</code> You need to have  the Docker CLI installed locally to use this backend.</p> <p>Authentication</p> <p>The Docker Daemon requires authentication to pull and push images from private registries. Run the  <code>docker login</code> command to authenticate.</p> <p>Authentication settings are stored in a <code>config.json</code> file located by default in <code>$HOME/.docker/</code>. If you need to provide a different configuration, you can set the <code>DOCKER_CONFIG</code> variable to the path to another  directory, which should contain a <code>config.json</code> file.</p> <p>Remote Daemon</p> <p>If you want to set a custom docker daemon host, you can set the <code>DOCKER_HOST</code> environment variable. The builds will then run on the remote host instead of using the local Docker daemon.</p> <p>BuildKit</p> <p>If available, dib will try to use the BuildKit engine to build images, which is faster than the default Docker build engine.</p>"},{"location":"backends/#kaniko","title":"Kaniko","text":"<p>Deprecated: The Kaniko backend is deprecated and will be removed in a future release. Please use the BuildKit backend instead.</p> <p>Kaniko offers a way to build container images inside a container  or Kubernetes cluster, without the security tradeoff of running a docker daemon container with host privileges.</p> <p>BuildKit</p> <p>As Kaniko must run in a container, it requires Docker when running local builds as it uses the <code>docker</code> executor.</p> <p>See the <code>kaniko</code> section in the configuration reference.</p>"},{"location":"backends/#buildkit","title":"BuildKit","text":"<p>BuildKit is a toolkit for converting source code to build artifacts in an efficient, expressive and repeatable manner. It provides a more efficient, cache-aware, and concurrent build engine compared to the traditional Docker build.</p> <p>Authentication</p> <p>BuildKit uses the same authentication mechanism as Docker. Run the <code>docker login</code> command to authenticate with your registry.</p> <p>Local Builds</p> <p>For local builds, BuildKit requires the <code>buildctl</code> binary to be installed on your system and <code>buildkitd</code> daemon to be running. You can install BuildKit by following the instructions in the official documentation.</p> <p>Kubernetes Builds</p> <p>For Kubernetes builds, dib will create a pod with the BuildKit image and execute the build inside it. This requires proper configuration of Kubernetes access and Docker registry credentials.</p> <p>BuildKit Host</p> <p>You can specify a custom BuildKit daemon host using the <code>--buildkit-host</code> option or by setting the <code>BUILDKIT_HOST</code> environment variable.</p> <p>See the <code>buildkit</code> section in the configuration reference.</p>"},{"location":"best-practices/","title":"dib Best Practices","text":""},{"location":"best-practices/#pin-dependencies-versions-in-dockerfiles","title":"Pin dependencies versions in Dockerfiles","text":"<p>As dib only rebuilds images when something changes in the build context (including the Dockerfile), external  dependencies should always be pinned to a specific version, so upgrading the dependency triggers a rebuild.</p> <p>Example: <pre><code>RUN apt-get install package@1.0.0\n</code></pre></p>"},{"location":"best-practices/#use-dockerignore","title":"Use .dockerignore","text":"<p>The <code>.dockerignore</code> lists file patterns that should not be included in the build context. dib also ignores those files when it computes the checksum, so no rebuild is triggered when they are modified.</p>"},{"location":"configuration-reference/","title":"Configuration Reference","text":"<pre><code>---\n# Log level: \"debug\", \"info\", \"warning\", \"error\", \"fatal\". Defaults to \"info\".\nlog_level: info\n\n# URL of the registry where the images should be stored.\n#\n# dib will use the local docker configuration to fetch metadata about existing images. You may use the DOCKER_CONFIG\n# environment variable to set a custom docker config path.\n# See the official Docker documentation (https://docs.docker.com/engine/reference/commandline/cli/#configuration-files).\n# The build backend must also be authenticated to have permission to push images.\nregistry_url: registry.example.org\n\n# The placeholder tag dib uses to mark which images are the reference. Defaults to \"latest\".\n# Change this value if you don't want to use \"latest\" tags, or if images may be tagged \"latest\" by other sources.\nplaceholder_tag: latest\n\n# The rate limit can be increased to allow parallel builds. This dramatically reduces the build times\n# when using the Kubernetes executor as build pods are scheduled across multiple nodes.\nrate_limit: 1\n\n# Use build arguments to set build-time variables. The format is a list of strings. Env vars are expanded.\nbuild_arg:\n  - FOO1=\"bar1\"\n  - FOO2=$BAR\n  - FOO3=${BAR}\n\n# Path to the directory where the reports are generated. The directory will be created if it doesn't exist.\nreports_dir: reports\n\n# The build backend. Can be set to \"buildkit\" (recommended), \"docker\" (deprecated), or \"kaniko\" (deprecated).\n#\n# Note: \"docker\" and \"kaniko\" backends are deprecated and will be removed in a future release.\n# The \"buildkit\" backend is now the recommended and default backend.\n#\n# Note: the kaniko backend must be run in a containerized environment such as Docker or Kubernetes.\n# See the \"executor\" section below.\nbackend: buildkit\n\n# BuildKit settings. Required when using the BuildKit backend.\nbuildkit:\n  # The build context directory has to be uploaded somewhere in order for the BuildKit pod to retrieve it,\n  # when using remote executor (Kubernetes). Currently, only AWS S3 is supported.\n  context:\n    # Store the build context in an AWS S3 bucket.\n    s3:\n      bucket: my-bucket\n      region: eu-west-3\n  # Executor configuration for Kubernetes.\n  executor:\n    # Configuration for the \"kubernetes\" executor.\n    kubernetes:\n      namespace: buildkit\n      image: moby/buildkit:latest\n      # References a secret containing the Docker configuration file used to authenticate to the registry.\n      docker_config_secret: docker-config-prod\n      env_secrets:\n        # Additional Secret mounted as environment variables.\n        # Used for instance to download the build context from AWS S3.\n        - aws-s3-secret\n      container_override: |\n        resources:\n          limits:\n            cpu: 2\n            memory: 8Gi\n          requests:\n            cpu: 1\n            memory: 2Gi\n      pod_template_override: |\n        spec:\n          affinity:\n            nodeAffinity:\n              requiredDuringSchedulingIgnoredDuringExecution:\n                nodeSelectorTerms:\n                - matchExpressions:\n                  - key: kops.k8s.io/instancegroup\n                    operator: In\n                    values:\n                    - spot-instances\n\n# Kaniko settings. Required only if using the Kaniko build backend (deprecated).\nkaniko:\n  # The build context directory has to be uploaded somewhere in order for the Kaniko pod to retrieve it,\n  # when using remote executor (Kuberentes or remote docker host). Currently, only AWS S3 is supported.\n  context:\n    # Store the build context in an AWS S3 bucket.\n    s3:\n      bucket: my-bucket\n      region: eu-west-3\n  # Executor configuration. It is only necessary to provide valid configurations for all of them,\n  # just pick one up according to your needs.\n  executor:\n    # Configuration for the \"docker\" executor.\n    docker:\n      image: eu.gcr.io/radio-france-k8s/kaniko:latest\n    # Configuration for the \"kubernetes\" executor.\n    kubernetes:\n      namespace: kaniko\n      image: eu.gcr.io/radio-france-k8s/kaniko:latest\n      # References a secret containing the Docker configuration file used to authenticate to the registry.\n      docker_config_secret: docker-config-prod\n      env_secrets:\n        # Additional Secret mounted as environment variables.\n        # Used for instance to download the build context from AWS S3.\n        - aws-s3-secret\n      container_override: |\n        resources:\n          limits:\n            cpu: 2\n            memory: 8Gi\n          requests:\n            cpu: 1\n            memory: 2Gi\n      pod_template_override: |\n        spec:\n          affinity:\n            nodeAffinity:\n              requiredDuringSchedulingIgnoredDuringExecution:\n                nodeSelectorTerms:\n                - matchExpressions:\n                  - key: kops.k8s.io/instancegroup\n                    operator: In\n                    values:\n                    - spot-instances\n\n# Enable test suites execution after each image build.\ninclude_tests:\n  # Enable Goss tests. See the \"goss\" configuration section below.\n  # To test an image, place a goss.yml file in its build context.\n  # Learn more about Goss: https://github.com/goss-org/goss\n  - goss\n  # Enable trivy vulnerability scans. See the \"trivy\" configuration section below.\n  # Learn more about Trivy: https://aquasecurity.github.io/trivy\n  - trivy\n\ngoss:\n  executor:\n    # Kubernetes executor configuration. Required when using the kubernetes build executor.\n    kubernetes:\n      enabled: true\n      namespace: goss\n      image: aelsabbahy/goss:latest\n      image_pull_secrets:\n      # - private-container-registry\n\ntrivy:\n  executor:\n    # Kubernetes executor configuration. Required when using the kubernetes build executor.\n    kubernetes:\n      enabled: true\n      namespace: trivy\n      image: ghcr.io/aquasecurity/trivy:latest\n      # References a secret containing the Docker configuration file used to authenticate to the registry.\n      docker_config_secret: docker-config-ci\n      image_pull_secrets:\n      # - private-container-registry\n      container_override: |\n        resources:\n          limits:\n            cpu: 2\n            memory: 3Gi\n          requests:\n            cpu: 2\n            memory: 1Gi\n        env:\n          - name: GOOGLE_APPLICATION_CREDENTIALS\n            value: /credentials/gcr_service_account.json\n          - name: TRIVY_TIMEOUT\n            value: \"30m0s\"\n        volumeMounts:\n          - mountPath: /credentials\n            name: private-registry-credentials\n            readOnly: true\n      pod_template_override: |\n        spec:\n          volumes:\n          - name: private-registry-credentials\n            secret:\n              defaultMode: 420\n              secretName: private-registry-credentials\n\n# Easter egg: A path to a file containing a custom wordlist that will be used to\n# generate the humanized hashes for image tags. The list must contain exactly 256 words.\n# You can enable the usage of this list in each Dockerfile with a custom label :\n#   LABEL dib.use-custom-hash-list=\"true\"\n# Please keep in mind each time you change this list the images using the\n# use-custom-hash-list label may see their hashes regenerated.\nhumanized_hash_list: \"\"\n# humanized_hash_list: \"custom_wordlist.txt\"\n</code></pre>"},{"location":"configuration/","title":"Configuration","text":"<p>dib can be configured either by command-line flags, environment variables or configuration file.</p> <p>The command-line flags have the highest priority, then environment variables, then config file. You can set some default values in the configuration file, and then override with environment variables of command-line flags.</p>"},{"location":"configuration/#command-line-flags","title":"Command-line flags","text":"<p>Example: <pre><code>dib build --registry-url=gcr.io/project\n</code></pre></p>"},{"location":"configuration/#environment-variables","title":"Environment variables","text":"<p>dib auto-discovers configuration from environment variables prefixed with <code>DIB_</code>, followed by the capitalized,  snake_cased flag name.</p> <p>Example: <pre><code>export DIB_REGISTRY_URL=gcr.io/project\ndib build\n</code></pre></p>"},{"location":"configuration/#configuration-file","title":"Configuration file","text":"<p>dib uses a YAML configuration file in addition to command-line arguments. It will look for a file names <code>.dib.yaml</code> in the current working directory. You can change the file location by setting the <code>--config</code> (<code>-c</code>) flag.</p> <p>The YAML keys are equivalent to the flag names, in snake_case.</p> <p>Example: <pre><code># .dib.yaml\nregistryUrl: gcr.io/project\n...\n</code></pre></p> <p>You can find more examples here. See also the  reference configuration file.</p>"},{"location":"documentation/","title":"Documentation","text":"<p>The documentation is generated with <code>mkdocs</code>. It generates a static website in plain HTML  from the Markdown files present in the <code>docs/</code> directory.</p> <p>We also use the Cobra built-in documentation generator for dib commands.</p>"},{"location":"documentation/#local-setup","title":"Local Setup","text":"<p>Let's set up a local Python environment and run the documentation server with live-reload.</p> <ol> <li> <p>Create a virtual env:     <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre></p> </li> <li> <p>Install dependencies:     <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Generate docs of dib commands:     <pre><code>make docs\n</code></pre></p> </li> <li> <p>Run the <code>mkdocs</code> server:     <pre><code>mkdocs serve\n</code></pre></p> </li> <li> <p>Go to http://localhost:8000</p> </li> </ol>"},{"location":"executors/","title":"Executors","text":"<p>dib supports multiple build executors. An executor is a platform able to run image builds and tests. Unlike the build backends which can be explicitely chosen, the executor is automatically selected depending on the type  of operation (build, test), and the executors configured in the configuration file.</p> <p>Build backend compatibility matrix</p> Executor BuildKit (recommended) Docker (deprecated) Kaniko (deprecated) Local \u2714 \u2714 \u2717 Docker \u2717 \u2717 \u2714 Kubernetes \u2714 \u2717 \u2714"},{"location":"executors/#local","title":"Local","text":"<p>Runs commands using the local exec system call. Use the <code>--local-only</code> flag to force the local executor.</p>"},{"location":"executors/#docker","title":"Docker","text":"<p>Runs commands in a docker container, using the <code>docker run</code> command.</p>"},{"location":"executors/#kubernetes","title":"Kubernetes","text":"<p>Creates pods in a kubernetes cluster, using the kubernetes API.  dib uses the current kube context, please make do</p> <p>See an example configuration in the  configuration reference section.</p>"},{"location":"extra-tags/","title":"Extra Tags","text":"<p>Images managed by dib will get tagged with the human-readable version of the computed hash. This is not very convenient in some cases, for instance if we want to tag an image with the explicit version of the contained software.</p> <p>dib allows additional tags to be definedusing a label in the Dockerfile: <pre><code>LABEL dib.extra-tags=\"v1.0.0,v1.0,v1\"\n</code></pre></p> <p>The label may contain a coma-separated list of tags to be created when the image gets promoted with the <code>--release</code> flag.</p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#how-to-run-dib-with-existing-containerd-standalone-or-created-by-docker","title":"How to run dib with existing containerd (standalone or created by docker)?","text":"<p>If you already have containerd running on your system (either standalone or as part of Docker), you can configure dib to use it through BuildKit. Here's how:</p> <ol> <li>First, make sure BuildKit is installed and running on your system. You can use this Docker image to help with that:</li> </ol> <pre><code>docker run --privileged --pid=host \\\n  -e CONTAINERD_ADDRESS=/run/containerd/containerd.sock \\\n  -e BUILDKIT_VERSION=v0.12.0 \\\n  &lt;buildkit-nsenter&gt;\n</code></pre> <ol> <li>Once BuildKit is running and connected to your containerd instance, you can configure dib to use it by setting the <code>buildkit_host</code> option in your configuration:</li> </ol> <pre><code># In .dib.yaml\nbuildkit_host: unix:///run/buildkit/buildkitd.sock\n</code></pre> <p>Or you can set it via environment variable:</p> <pre><code>export DIB_BUILDKIT_HOST=unix:///run/buildkit/buildkitd.sock\n</code></pre> <ol> <li>Now when you run dib, it will use your existing BuildKit daemon, which is connected to your containerd instance:</li> </ol> <pre><code>dib build\n</code></pre> <p>This approach gives you the best of both worlds - you can use dib's powerful image building capabilities while leveraging your existing containerd setup for efficient container operations.</p>"},{"location":"install/","title":"Installation Guide","text":"Install with goFrom binary <p>Install the latest release on macOS or Linux with:</p> <pre><code>go install github.com/radiofrance/dib@latest\n</code></pre> <p>Binaries are available to download from the GitHub releases page.</p>"},{"location":"install/#shell-autocompletion","title":"Shell autocompletion","text":"<p>Configure your shell to load dib completions:</p> BashFishPowershellZsh <p>To load completion run:</p> <pre><code>. &lt;(dib completion bash)\n</code></pre> <p>To configure your bash shell to load completions for each session add to your bashrc:</p> <pre><code># ~/.bashrc or ~/.bash_profile\ncommand -v dib &gt;/dev/null &amp;&amp; . &lt;(dib completion bash)\n</code></pre> <p>If you have an alias for dib, you can extend shell completion to work with that alias:</p> <pre><code># ~/.bashrc or ~/.bash_profile\nalias tm=dib\ncomplete -F __start_dib tm\n</code></pre> <p>To configure your fish shell to load completions for each session write this script to your completions dir:</p> <pre><code>dib completion fish &gt; ~/.config/fish/completions/dib.fish\n</code></pre> <p>To load completion run:</p> <pre><code>. &lt;(dib completion powershell)\n</code></pre> <p>To configure your powershell shell to load completions for each session add to your powershell profile:</p> <p>Windows:</p> <p><pre><code>cd \"$env:USERPROFILE\\Documents\\WindowsPowerShell\\Modules\"\ndib completion &gt;&gt; dib-completion.ps1\n</code></pre> Linux:</p> <pre><code>cd \"${XDG_CONFIG_HOME:-\"$HOME/.config/\"}/powershell/modules\"\ndib completion &gt;&gt; dib-completions.ps1\n</code></pre> <p>To load completion run:</p> <pre><code>. &lt;(dib completion zsh) &amp;&amp; compdef _dib dib\n</code></pre> <p>To configure your zsh shell to load completions for each session add to your zshrc:</p> <pre><code># ~/.zshrc or ~/.profile\ncommand -v dib &gt;/dev/null &amp;&amp; . &lt;(dib completion zsh) &amp;&amp; compdef _dib dib\n</code></pre> <p>or write a cached file in one of the completion directories in your ${fpath}:</p> <pre><code>echo \"${fpath// /\\n}\" | grep -i completion\ndib completion zsh &gt; _dib\n\nmv _dib ~/.oh-my-zsh/completions  # oh-my-zsh\nmv _dib ~/.zprezto/modules/completion/external/src/  # zprezto\n</code></pre>"},{"location":"quickstart/","title":"Quickstart Guide","text":"<p>This guide will show you the basics of dib. You will build a set of images locally using the local docker daemon.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before using dib, ensure you have the following dependencies installed:</p> <ul> <li>Docker for building images on your local computer.</li> <li>Graphviz for generating visual representation of the dependency graph (optional)</li> <li>Goss for testing images after build (optional)</li> <li>Trivy for scanning images for vulnerabilites (optional)</li> </ul> <p>Then, you need to install the dib command-line by following the installation guide.</p> <p>Make sure you have authenticated access to an OCI registry, in this guide we'll assume it is <code>registry.example.com</code>.</p>"},{"location":"quickstart/#directory-structure","title":"Directory structure","text":"<p>Let's create a root directory containing 2 Dockerfiles in their own subdirectories. The structure will look like: <pre><code>docker/\n\u251c\u2500\u2500 base\n|   \u2514\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 child\n    \u2514\u2500\u2500 Dockerfile\n</code></pre></p> <p>Now create the dockerfile for the <code>base</code> image: <pre><code># docker/base/Dockerfile\nFROM alpine:latest\n\nLABEL name=\"base\"\n</code></pre></p> <p>The \"name\" label is mandatory, it is used by dib to name the current image, by appending the value of the label to the  registry URL. In this case, the image name is <code>registry.example.com/base</code>.</p> <p>Then, create the dockerfile for the <code>child</code> image, which extends the <code>base</code> image: <pre><code># docker/child/Dockerfile\nFROM registry.example.com/base:latest\n\nLABEL name=\"child\"\n</code></pre></p> <p>Tip</p> <p>The directory structure does not matter to dib. It builds the graph of dependencies based on the FROM statements. You can have either flat directory structure like shown above, or embed child images context directories in the parent context.</p>"},{"location":"quickstart/#configuration","title":"Configuration","text":"<p>See the configuration section </p> <p>For this guide, we'll use a configuration file as it is the more convenient way for day-to-day usage.</p> <p>Let's create a <code>.dib.yaml</code> next to the docker build directory: <pre><code>docker/\n\u251c\u2500\u2500 base/\n\u251c\u2500\u2500 child/\n\u2514\u2500\u2500 .dib.yaml\n</code></pre></p> <p>Edit the file to set the registry name, used to pull and push dib-managed images. <pre><code>registry_url: registry.example.com\n</code></pre></p> <p>You can check everything is correct by running <code>dib list</code>: <pre><code>$ dib list\nUsing config file: docs/examples/.dib.yaml\n  NAME   HASH\n  base   august-berlin-blossom-magnesium\n  child  gee-minnesota-maryland-robin\n</code></pre></p> <p>You should get the output containing the list of images that dib has discovered.</p>"},{"location":"quickstart/#building-the-images","title":"Building the images","text":"<p>When you have all your images definitions in the build directory and configuration set up, you can proceed to building  the images: <pre><code>$ dib build\n...\n</code></pre></p> <p>When it's done, you can run the build command again, and you'll see that dib does nothing as long as the Dockerfiles  remain unchanged.</p> <p>When you are ready to promote the images to <code>latest</code>, run: <pre><code>$ dib build --release\n</code></pre></p>"},{"location":"reports/","title":"Reporting","text":"<p>dib generates reports after each build. By default, the reports are generated in the <code>reports</code> directory. You can change it by setting the <code>--reports-dir</code> option to another location.</p>"},{"location":"reports/#html-report","title":"HTML Report","text":"<p>The HTML report is the one you are going to use the most. Just click on the link displayed on the dib output to browse the report.</p> <p>In the report you'll find:</p> <ul> <li>An overview of all images managed by dib</li> <li>The build output</li> <li>The graph of dependencies</li> <li>Test results and logs</li> <li>Vulnerability scan results</li> </ul> <p>Preview:</p> <p></p>"},{"location":"reports/#junit-reports","title":"jUnit Reports","text":"<p>Test executors generate reports in jUnit format.  They can then be parsed in a CI pipeline and displayed in a user-friendly fashion.</p>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#road-to-v1","title":"Road to v1","text":"<p>dib is still a work in progress, but we plan to release a stable version (v1.0.0) after we have added the  following features:</p> <ul> <li>Per-image configuration: Some images may require their own tagging scheme. Being      able to configure those settings for each image is necessary.</li> </ul>"},{"location":"roadmap/#future-additions","title":"Future additions","text":"<ul> <li>Multiplatform builds: Ability to build images for different platforms, and generate a manifest-list.</li> <li>Image signing: Sign images to improve supply chain security.</li> </ul> <p>And more...</p>"},{"location":"tests/","title":"Tests","text":"<p>dib can execute tests suites to make assertions on images that it just built. This is useful to prevent regressions,  and ensure everything work as expected at runtime.</p>"},{"location":"tests/#goss","title":"Goss","text":"<p>Goss is a YAML-based serverspec alternative tool for validating a server\u2019s configuration. dib runs a container from the  image to test, and injects the goss binary and configuration, then execute the test itself.</p> <p>To get started with goss tests, follow the steps below:</p> <ol> <li> <p>Install goss locally (for local builds only)</p> <p>Follow the procedure from the official docs</p> </li> <li> <p>Ensure the goss tests are enabled in configuration:     <pre><code># .dib.yaml\ninclude_tests:\n  - goss\n</code></pre></p> </li> <li> <p>Create a <code>goss.yaml</code> file next to the Dockerfile of the image to test     <pre><code>debian/\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 goss.yaml\n</code></pre></p> </li> <li> <p>Add some assertions in the <code>goss.yaml</code>     Basic Example:     <pre><code>command:\n  'check \"hello-world\" version':\n    exec: 'hello-world --version'\n    exit-status: 0\n    stdout:\n    - '/^hello-world version [0-9]+\\.[0-9]+\\.[0-9]+ linux\\/amd64$/'\n</code></pre></p> </li> </ol> <p>Read the Goss documentation to learn all possible assertions.</p>"},{"location":"cmd/dib/","title":"dib","text":""},{"location":"cmd/dib/#dib","title":"dib","text":"<p>An Opinionated DAG Image Builder</p>"},{"location":"cmd/dib/#synopsis","title":"Synopsis","text":"<p>DAG Image Builder helps building a complex image dependency graph</p> <p>Run dib --help for more information</p>"},{"location":"cmd/dib/#options","title":"Options","text":"<pre><code>      --build-path string            Path to the directory containing all Dockerfiles to be built by dib. Every Dockerfile will be recursively \n                                     found and added to the build graph. You can provide any subdirectory if you want to focus on a reduced set of images, \n                                     as long as it has at least one Dockerfile in it. (default \"docker\")\n      --config string                config file (default is $HOME/.config/.dib.yaml)\n      --hash-list-file-path string   Path to custom hash list file that will be used to humanize hash\n  -h, --help                         help for dib\n  -l, --log-level string             Log level. Can be any standard log-level (\"info\", \"debug\", etc...) (default \"info\")\n      --placeholder-tag string       Tag used as placeholder in Dockerfile \"from\" statements, and replaced internally by dib during builds \n                                     to use the latest tags from parent images. In release mode, all images will be tagged with the placeholder tag, so \n                                     Dockerfiles are always valid (images can still be built even without using dib). (default \"latest\")\n      --registry-url string          Docker registry URL where images are stored. (default \"eu.gcr.io/my-test-repository\")\n</code></pre>"},{"location":"cmd/dib/#see-also","title":"SEE ALSO","text":"<ul> <li>dib build     - Run oci images builds</li> <li>dib list   - List all images managed by dib</li> <li>dib version     - print current dib version</li> </ul>"},{"location":"cmd/dib/#auto-generated-by-spf13cobra-on-31-dec-2025","title":"Auto generated by spf13/cobra on 31-Dec-2025","text":""},{"location":"cmd/dib_build/","title":"dib build","text":""},{"location":"cmd/dib_build/#dib-build","title":"dib build","text":"<p>Run oci images builds</p>"},{"location":"cmd/dib_build/#synopsis","title":"Synopsis","text":"<p>dib build will compute the graph of images, and compare it to the last built state.</p> <p>For each image, if any file part of its docker context has changed, the image will be rebuilt. Otherwise, dib will create a new tag based on the previous tag.</p> <pre><code>dib build [flags]\n</code></pre>"},{"location":"cmd/dib_build/#options","title":"Options","text":"<pre><code>  -b, --backend string             Build Backend used to run image builds. Supported backends: [docker kaniko buildkit] (docker and kaniko are deprecated) (default \"buildkit\")\n      --build-arg argument=value   argument=value to supply to the builder\n      --buildkit-host string       buildkit host address.\n      --dry-run                    Simulate what would happen without actually doing anything dangerous.\n  -f, --file string                Name of the Dockerfile\n      --force-rebuild              Forces rebuilding the entire image graph, without regarding if the target version already exists.\n  -h, --help                       help for build\n      --include-tests strings      List of test runners to exclude during the test phase.\n      --local-only                 Build Docker images locally. If this flag is not set, the build will be performed in Kubernetes.\n      --no-graph                   Disable generation of graph during the build process.\n      --no-retag                   Disable re-tagging images after build. Note that temporary tags with the \"dev-\" prefix may still be pushed to the registry.\n      --no-tests                   Disable execution of tests (unit tests, scans, etc...) after the build.\n      --progress string            Set type of progress output (auto, plain, tty). Use plain to show container output (default \"auto\")\n      --push                       Push the images to the registry after building them.\n      --rate-limit int             Concurrent number of builds that can run simultaneously (default 1)\n      --release dib.extra-tags     Enable release mode to tag all images with extra tags found in the dib.extra-tags Dockerfile labels.\n      --reports-dir string         Path to the directory where the reports are generated. (default \"reports\")\n      --target string              Set the target build stage to build (applies to all Dockerfiles managed by dib)\n</code></pre>"},{"location":"cmd/dib_build/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --build-path string            Path to the directory containing all Dockerfiles to be built by dib. Every Dockerfile will be recursively \n                                     found and added to the build graph. You can provide any subdirectory if you want to focus on a reduced set of images, \n                                     as long as it has at least one Dockerfile in it. (default \"docker\")\n      --config string                config file (default is $HOME/.config/.dib.yaml)\n      --hash-list-file-path string   Path to custom hash list file that will be used to humanize hash\n  -l, --log-level string             Log level. Can be any standard log-level (\"info\", \"debug\", etc...) (default \"info\")\n      --placeholder-tag string       Tag used as placeholder in Dockerfile \"from\" statements, and replaced internally by dib during builds \n                                     to use the latest tags from parent images. In release mode, all images will be tagged with the placeholder tag, so \n                                     Dockerfiles are always valid (images can still be built even without using dib). (default \"latest\")\n      --registry-url string          Docker registry URL where images are stored. (default \"eu.gcr.io/my-test-repository\")\n</code></pre>"},{"location":"cmd/dib_build/#see-also","title":"SEE ALSO","text":"<ul> <li>dib     - An Opinionated DAG Image Builder</li> </ul>"},{"location":"cmd/dib_build/#auto-generated-by-spf13cobra-on-31-dec-2025","title":"Auto generated by spf13/cobra on 31-Dec-2025","text":""},{"location":"cmd/dib_list/","title":"dib list","text":""},{"location":"cmd/dib_list/#dib-list","title":"dib list","text":"<p>List all images managed by dib</p>"},{"location":"cmd/dib_list/#synopsis","title":"Synopsis","text":"<p>Command list provide different ways to print the list of all Docker images managed by dib.</p> <p>The output can be customized with the --output flag : \u2022 console (default output)   ex : dib list</p> <p>\u2022 go-template-file (render output using a Go template)   ex : dib list -o go-template-file=dib_list.tmpl</p> <p>\u2022 graphviz (dot language output)   ex : dib list -o graphviz</p> <p>You can also generate a PNG image from the graphviz output using the following command :   dib list -o graphviz | dot -Tpng &gt; dib.png</p> <pre><code>dib list [flags]\n</code></pre>"},{"location":"cmd/dib_list/#options","title":"Options","text":"<pre><code>      --build-arg argument=value   argument=value to supply to the builder\n  -h, --help                       help for list\n  -o, --output string              Output format : console|graphviz|go-template-file (default \"console\")\n</code></pre>"},{"location":"cmd/dib_list/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --build-path string            Path to the directory containing all Dockerfiles to be built by dib. Every Dockerfile will be recursively \n                                     found and added to the build graph. You can provide any subdirectory if you want to focus on a reduced set of images, \n                                     as long as it has at least one Dockerfile in it. (default \"docker\")\n      --config string                config file (default is $HOME/.config/.dib.yaml)\n      --hash-list-file-path string   Path to custom hash list file that will be used to humanize hash\n  -l, --log-level string             Log level. Can be any standard log-level (\"info\", \"debug\", etc...) (default \"info\")\n      --placeholder-tag string       Tag used as placeholder in Dockerfile \"from\" statements, and replaced internally by dib during builds \n                                     to use the latest tags from parent images. In release mode, all images will be tagged with the placeholder tag, so \n                                     Dockerfiles are always valid (images can still be built even without using dib). (default \"latest\")\n      --registry-url string          Docker registry URL where images are stored. (default \"eu.gcr.io/my-test-repository\")\n</code></pre>"},{"location":"cmd/dib_list/#see-also","title":"SEE ALSO","text":"<ul> <li>dib     - An Opinionated DAG Image Builder</li> </ul>"},{"location":"cmd/dib_list/#auto-generated-by-spf13cobra-on-31-dec-2025","title":"Auto generated by spf13/cobra on 31-Dec-2025","text":""},{"location":"cmd/dib_version/","title":"dib version","text":""},{"location":"cmd/dib_version/#dib-version","title":"dib version","text":"<p>print current dib version</p> <pre><code>dib version [flags]\n</code></pre>"},{"location":"cmd/dib_version/#options","title":"Options","text":"<pre><code>  -h, --help   help for version\n</code></pre>"},{"location":"cmd/dib_version/#options-inherited-from-parent-commands","title":"Options inherited from parent commands","text":"<pre><code>      --build-path string            Path to the directory containing all Dockerfiles to be built by dib. Every Dockerfile will be recursively \n                                     found and added to the build graph. You can provide any subdirectory if you want to focus on a reduced set of images, \n                                     as long as it has at least one Dockerfile in it. (default \"docker\")\n      --config string                config file (default is $HOME/.config/.dib.yaml)\n      --hash-list-file-path string   Path to custom hash list file that will be used to humanize hash\n  -l, --log-level string             Log level. Can be any standard log-level (\"info\", \"debug\", etc...) (default \"info\")\n      --placeholder-tag string       Tag used as placeholder in Dockerfile \"from\" statements, and replaced internally by dib during builds \n                                     to use the latest tags from parent images. In release mode, all images will be tagged with the placeholder tag, so \n                                     Dockerfiles are always valid (images can still be built even without using dib). (default \"latest\")\n      --registry-url string          Docker registry URL where images are stored. (default \"eu.gcr.io/my-test-repository\")\n</code></pre>"},{"location":"cmd/dib_version/#see-also","title":"SEE ALSO","text":"<ul> <li>dib     - An Opinionated DAG Image Builder</li> </ul>"},{"location":"cmd/dib_version/#auto-generated-by-spf13cobra-on-31-dec-2025","title":"Auto generated by spf13/cobra on 31-Dec-2025","text":""}]}